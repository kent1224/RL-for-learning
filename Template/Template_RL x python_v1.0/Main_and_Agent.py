# -*- coding: utf-8 -*-
"""
Created on Fri Apr 21 11:10:18 2017

@author: 14224
"""
import random
from Environment import Environment

class LearningAgent(Agent):
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """ 
    
    def __init__(self, env, learning = False, epsilon = 1.0, alpha = 0.9):
        """ Initializes an instance of a class or an object.
            self variable represents the instance of the object itself. In python, you have to declare it explicitly"""
        super(LearningAgent, self).__init__(env)     # Set the agent in the evironment
                
        self.valid_actions = self.env.valid_actions  # The set of valid actions        
        
        # Set parameters of the learning agent
        self.learning = learning # Whether the agent is expected to learn
        self.Q = dict()          # Create a Q-table which will be a dictionary of tuples
        self.epsilon = epsilon   # Random exploration factor
        self.alpha = alpha       # Learning factor
    
    
    def reset(self, destination=None, ):
        """ Epsilon, Alpha, Gamma"""
        """ Reset the destination and alpha, epsilon, ..."""    
    
    
    def build_state(self,):
        """ State space"""
        """ The build_state function is called when the agent requests data from the 
            environment. The next waypoint, the intersection inputs, and the deadline 
            are all features available to the agent. """
            
    
    def createQ(self, state):
        """Build a new state dictionary in Q-Table """
        """ The createQ function is called when a state is generated by the agent. """
        # When learning, check if the 'state' is not in the Q-table
        # If it is not, create a new dictionary for that state
        #   Then, for each action available, set the initial Q-value to 0.0
        if self.learning and state not in self.Q:
            self.Q[state] = {'left': 0, 'right':0, 'forward':0, None:0}
        return 
    
        # Q-Table is look like this:
        #    { 'state-1': { 
        #        'action-1' : Qvalue-1,
        #        'action-2' : Qvalue-2,
        #         ...
        #       },
        #      'state-2': {
        #        'action-1' : Qvalue-1,
        #         ...
        #       },
        #       ...
        #    }
    

        
    def get_maxQ(self, state):
        """ Find the Max Q-value """
        """ The get_max_Q function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """
        
        # Calculate the maximum Q-value of all actions for a given state
        maxAction = max(self.Q[state], key = lambda x: self.Q[state][x])
        maxQ = self.Q[state][maxAction]
        return maxQ
    
    
    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """
            
        # Set the agent state and default action
        
        
        
        # When not learning, choose a random action
        action = random.choice(self.valid_actions)
        
        # When learning, choose a random action with 'epsilon' probability
        #   Otherwise, choose an action with the highest Q-value for the current state
        if self.learning and random.random() > self.epsilon:
            maxVal = self.get_maxQ(state)

            #choose randomly if there are more than one max values
            pos_actions = [act for act in self.Q[state] if self.Q[state][act] == maxVal]
            action = random.choice(pos_actions)
        return action

    def update(self):
        """ The update function is called when a time step is completed in the 
            environment for a given trial. This function will build the agent
            state, choose an action, receive a reward, and learn if enabled. """
        
        state = self.build_state()          # Get current state
        self.createQ(state)                 # Create 'state' in Q-table
        action = self.choose_action(state)  # Choose an action
        reward = self.env.act(self, action) # Receive a reward
        self.learn(state, action, reward)   # Q-learn

        return
    
    def learn(self, state, action, reward):
        """To learn """
        """ The learn function is called after the agent completes an action and
            receives an award. This function does not consider future rewards 
            when conducting learning. """
        
        # When learning, implement the value iteration update rule
        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')    
        old_value = self.Q[state][action]
        if self.learning:
            self.Q[state][action] = (1-self.alpha)*old_value + self.alpha*(reward)

        return

def run():
    """ Main """
    """ Driving function for running the simulation. 
        Press ESC to close the simulation, or [SPACE] to pause the simulation. """
    
    # Create the environment
    env = Environment()    
    # Create the driving agent
    agent = env.create_agent(LearningAgent, learning=True)
    # Follow the driving agent
    
    # Create the simulation
    
    # Run the simulator
        
        

if __name__ == '__main__':
    run()